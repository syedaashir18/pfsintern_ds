# -*- coding: utf-8 -*-
"""CREDIT_CARD.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KvuCWokG7kWn4L0oyi5n45_RrKAF4Hzy
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")

data=pd.read_csv("Credit_card.csv")
data

"""Data Exploration"""

data.head()

data.isnull().sum()

data.info()

data.describe()

data.shape

data['Class'].value_counts()

print('No Frauds', round(data['Class'].value_counts()[0]/len(data) * 100,2), '% of the dataset')
print('Frauds', round(data['Class'].value_counts()[1]/len(data) * 100,2), '% of the dataset')

# explore amount of No-fraud transaction
NoFraud=data[data["Class"]==0]
NoFraud.Amount.describe()

# explore amount of Fraud transaction
Fraud=data[data["Class"]==1]
Fraud.Amount.describe()

data.groupby('Class').mean()

"""Exploratory Data Analysis (EDA)"""

# distribution of Non-Fraud and Fraud Transactions
plt.figure(figsize=(6, 4))
sns.countplot(x='Class', data=data, palette={'0.0': 'cornflowerblue', '1.0': 'red'}) # Changed the keys in the palette dictionary from integers to strings to match the values in the DataFrame.
plt.xlabel('Transaction Type')
plt.ylabel('Count')
plt.title('Count of Fraud and Non-Fraud Transactions')
plt.xticks(ticks=[0, 1], labels=['Non-Fraud', 'Fraud'])
plt.show()

# distributions of transaction time and transaction amount
f, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
ax1 = sns.distplot(data['Time'], ax=ax1, color='g')
ax2 = sns.distplot(data['Amount'], ax=ax2, color='salmon')
ax1.set_title('Distribution of Transaction Time', fontsize=13)
ax2.set_title('Distribution of Transaction Amount', fontsize=13)

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(12,6))
s = sns.boxplot(ax = ax1, x="Class", y="Amount", hue="Class",data=data, palette="PRGn",showfliers=True)
s = sns.boxplot(ax = ax2, x="Class", y="Amount", hue="Class",data=data, palette="PRGn",showfliers=False)
plt.show()

plt.figure(figsize=(7,4))
sns.kdeplot(data=data[data['Class'] == 0], x='Time', label='Non-Fraud', color='blue', fill=True)
sns.kdeplot(data=data[data['Class'] == 1], x='Time', label='Fraud', color='red', fill=True)
plt.xlabel('Time')
plt.ylabel('Density')
plt.title('Density Plot of Fraud and Non-Fraud Transactions with Respect to Time')
plt.legend()
plt.show()

data1=data.copy()
data1['Hour'] = data1['Time'].apply(lambda x: np.floor(x / 3600))

tmp = data1.groupby(['Hour', 'Class'])['Amount'].aggregate(['min', 'max', 'count', 'sum', 'mean', 'median', 'var']).reset_index()
data1 = pd.DataFrame(tmp)
data1.columns = ['Hour', 'Class', 'Min', 'Max', 'Transactions', 'Sum', 'Mean', 'Median', 'Var']
data1.head()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Sum", data=data1.loc[data1.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Sum", data=data1.loc[data1.Class==1], color="red")
plt.suptitle("Total Amount")
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Mean", data=data1.loc[data1.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Mean", data=data1.loc[data1.Class==1], color="red")
plt.suptitle("Average Amount of Transactions")
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Transactions", data=data1.loc[data1.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Transactions", data=data1.loc[data1.Class==1], color="red")
plt.suptitle("Total Number of Transactions")
plt.show()

fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(18,6))
s = sns.lineplot(ax = ax1, x="Hour", y="Mean", data=data1.loc[data1.Class==0])
s = sns.lineplot(ax = ax2, x="Hour", y="Mean", data=data1.loc[data1.Class==1], color="red")
plt.suptitle("Average Amount of Transactions")
plt.show()

data.hist(figsize=(20,20))
plt.show()

# correlation matrix
corr = data.corr()
sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':20})

"""Scaling"""

from sklearn.preprocessing import StandardScaler, RobustScaler

std_scaler = StandardScaler()
rob_scaler = RobustScaler()

data['scaled_amount'] = rob_scaler.fit_transform(data['Amount'].values.reshape(-1,1))
data['scaled_time'] = rob_scaler.fit_transform(data['Time'].values.reshape(-1,1))

data.drop(['Time','Amount'], axis=1, inplace=True)

scaled_amount = data['scaled_amount']
scaled_time = data['scaled_time']

data.drop(['scaled_amount', 'scaled_time'], axis=1, inplace=True)
data.insert(0, 'scaled_amount', scaled_amount)
data.insert(1, 'scaled_time', scaled_time)

data.head()

"""Random Under-Sampling"""

df = data.copy()
df = df.sample(frac=1)
# amount of fraud classes 492 rows.
fraud_df= df.loc[df['Class'] == 1]
non_fraud_df = df.loc[df['Class'] == 0][:492]

normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

# Shuffle dataframe rows
new_df = normal_distributed_df.sample(frac=1, random_state=42)
new_df.head()

new_df['Class'].value_counts()

plt.figure(figsize=(6, 4))
sns.countplot(x='Class', data=new_df, palette={'0.0': 'cornflowerblue', '1.0': 'lightcoral'}) # Changed the keys in the palette dictionary to '0.0' and '1.0' to match the data type of the 'Class' column.
plt.xlabel('Transaction Type')
plt.ylabel('Count')
plt.title('Count of Fraud and Non-Fraud Transactions (Equally distributed classes)')
plt.xticks(ticks=[0, 1], labels=['Non-Fraud', 'Fraud'])
plt.show()

"""Model building"""

X=new_df.drop(columns='Class',axis=1)
y=new_df['Class']

X.head()

y.head()

from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

classifiers = {
    "LogisticRegression": LogisticRegression(),
    "Support Vector Classifier": SVC(),
    "DecisionTreeClassifier": DecisionTreeClassifier(),
    "RandomForestClassifier": RandomForestClassifier(),
    "BaggingClassifier": BaggingClassifier(n_estimators=10, random_state=0),
    "GradientBoostingClassifier": GradientBoostingClassifier(),
}

"""Accuracy and Classification report"""

def evaluate_classifiers(X_train, y_train, X_test, y_test, classifiers):
    results = {}

    for name, clf in classifiers.items():
        clf.fit(X_train, y_train)
        y_pred = clf.predict(X_test)

        # Calculate accuracy and classification report
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        results[name] = {
            'accuracy': accuracy,
            'classification_report': report
        }

        print(f"\n{name} Accuracy: {accuracy:.4f}\n")
        print(classification_report(y_test, y_pred))
        print("-----------------------------------------------------")

    return results

results = evaluate_classifiers(X_train, y_train, X_test, y_test, classifiers)

"""Plot Confusion Matrix"""

# Initialize classifiers
log_reg = LogisticRegression()
svc = SVC(probability=True)  # Ensure you have probability=True for ROC
tree_clf = DecisionTreeClassifier()
rf = RandomForestClassifier()
bag = BaggingClassifier(n_estimators=10, random_state=0)
gb = GradientBoostingClassifier()

# Fit classifiers
log_reg.fit(X_train, y_train)
svc.fit(X_train, y_train)
tree_clf.fit(X_train, y_train)
rf.fit(X_train, y_train)
bag.fit(X_train, y_train)
gb.fit(X_train, y_train)

# Generate predictions
y_pred_log_reg = log_reg.predict(X_test)
y_pred_svc = svc.predict(X_test)
y_pred_tree = tree_clf.predict(X_test)
y_pred_rf = rf.predict(X_test)
y_pred_bag = bag.predict(X_test)
y_pred_gb = gb.predict(X_test)

# Compute confusion matrices
log_reg_cf = confusion_matrix(y_test, y_pred_log_reg)
svc_cf = confusion_matrix(y_test, y_pred_svc)
tree_cf = confusion_matrix(y_test, y_pred_tree)
rf_cf = confusion_matrix(y_test, y_pred_rf)
bag_cf = confusion_matrix(y_test, y_pred_bag)
gb_cf = confusion_matrix(y_test, y_pred_gb)

# Plot confusion matrices
fig, ax = plt.subplots(2, 3, figsize=(22, 12))

sns.heatmap(log_reg_cf, ax=ax[0][0], annot=True, cmap=plt.cm.copper, fmt='g')
ax[0, 0].set_title("Logistic Regression \n Confusion Matrix", fontsize=14)
ax[0, 0].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[0, 0].set_yticklabels(['', ''], fontsize=12, rotation=360)

sns.heatmap(svc_cf, ax=ax[0][1], annot=True, cmap=plt.cm.copper, fmt='g')
ax[0, 1].set_title("Support Vector Classifier \n Confusion Matrix", fontsize=14)
ax[0, 1].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[0, 1].set_yticklabels(['', ''], fontsize=12, rotation=360)

sns.heatmap(tree_cf, ax=ax[0][2], annot=True, cmap=plt.cm.copper, fmt='g')
ax[0, 2].set_title("Decision Tree Classifier \n Confusion Matrix", fontsize=14)
ax[0, 2].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[0, 2].set_yticklabels(['', ''], fontsize=12, rotation=360)

sns.heatmap(rf_cf, ax=ax[1][0], annot=True, cmap=plt.cm.copper, fmt='g')
ax[1, 0].set_title("Random Forest Classifier \n Confusion Matrix", fontsize=14)
ax[1, 0].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[1, 0].set_yticklabels(['', ''], fontsize=12, rotation=360)

sns.heatmap(bag_cf, ax=ax[1][1], annot=True, cmap=plt.cm.copper, fmt='g')
ax[1, 1].set_title("Bagging Classifier \n Confusion Matrix", fontsize=14)
ax[1, 1].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[1, 1].set_yticklabels(['', ''], fontsize=12, rotation=360)

sns.heatmap(gb_cf, ax=ax[1][2], annot=True, cmap=plt.cm.copper, fmt='g')
ax[1, 2].set_title("Gradient Boosting Classifier \n Confusion Matrix", fontsize=14)
ax[1, 2].set_xticklabels(['', ''], fontsize=12, rotation=90)
ax[1, 2].set_yticklabels(['', ''], fontsize=12, rotation=360)

plt.tight_layout()
plt.show()

"""ROC Curve"""

from sklearn.metrics import roc_curve, roc_auc_score
from sklearn.model_selection import cross_val_predict

plt.figure(figsize=(10, 8))

for name, clf in classifiers.items():
    # Perform cross-validation and get the prediction probabilities
    if hasattr(clf, "decision_function"):
        y_scores = cross_val_predict(clf, X_train, y_train, cv=5, method="decision_function")
    else:
        y_scores = cross_val_predict(clf, X_train, y_train, cv=5, method="predict_proba")[:, 1]

    # Calculate ROC curve
    fpr, tpr, _ = roc_curve(y_train, y_scores)
    roc_auc = roc_auc_score(y_train, y_scores)

    # Plot ROC curve
    plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')

# Plot the baseline
plt.plot([0, 1], [0, 1], 'k--', label='Baseline (AUC = 0.50)')


plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curves for Different Classifiers')
plt.legend(loc='best')
plt.grid()

plt.show()

