# -*- coding: utf-8 -*-
"""Email Spam.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JnkPFZ06agGY_gfy_Y8fwcRBwQJ_rlIf

Loading Packages
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.preprocessing import LabelEncoder

"""Data Overview"""

data=pd.read_csv('Email Spam.csv',encoding='cp1252')

data.head()

data.shape

"""Data Cleaning"""

data.isnull().sum()

data.info()

data.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],inplace=True)

data.rename(columns={'v1':'Label','v2':'Text'},inplace=True)

data.head()

data.groupby("Label").count()

"""Dealing with duplicate values"""

data.duplicated().sum()

data = data.drop_duplicates(keep="first")

data.shape

"""Label Encoding"""

from sklearn.preprocessing import LabelEncoder
encoder = LabelEncoder()

data['Label'] = encoder.fit_transform(data['Label'])
data.head()

"""EDA"""

data['Label'].value_counts().plot(kind='bar')

import matplotlib.pyplot as plt
plt.pie(data['Label'].value_counts(), labels=['ham', 'spam'], autopct="%0.2f")
plt.show()

"""Feature Engineering: Adding Text-related Columns for Character, Word, and Sentence CountsÂ¶"""

import nltk

import nltk

# Download the 'punkt' resource
nltk.download('punkt')

# num of characters
data['num_characters'] = data['Text'].apply(len)
# num of words
data['num_words'] = data['Text'].apply(lambda x:len(nltk.word_tokenize(x)))
# num of sentences
data['num_sentences'] = data['Text'].apply(lambda x:len(nltk.sent_tokenize(x)))

data.head()

"""Lets understand the range and variability of character counts, word counts, and sentence counts in the text data."""

data[['num_characters','num_words','num_sentences']].describe()

# ham
data[data['Label'] == 0][['num_characters','num_words','num_sentences']].describe()

# spam
data[data['Label'] == 1][['num_characters','num_words','num_sentences']].describe()

"""Lets visualize"""

# num_characters
import seaborn as sns
sns.histplot(data[data['Label'] == 0]['num_characters'], label='Non-spam')
sns.histplot(data[data['Label'] == 1]['num_characters'], label='Spam')

plt.xlabel('Number of Characters')
plt.ylabel('Frequency')
plt.title('Histogram of Number of Characters')
plt.legend()
plt.show()

#num_words
sns.histplot(data[data['Label'] == 0]['num_words'], label='Non-spam')
sns.histplot(data[data['Label'] == 1]['num_words'], label='Spam')

plt.xlabel('Number of words')
plt.ylabel('Frequency')
plt.title('Histogram of Number of words')
plt.legend()
plt.show()

fig, axes = plt.subplots(1, 2, figsize=(10, 5))

# Plot for non-spam messages
sns.histplot(data[data['Label'] == 0]['num_sentences'], ax=axes[0])
axes[0].set_xlabel('Number of Sentences')
axes[0].set_ylabel('Frequency')
axes[0].set_title('Histogram of Number of Sentences (ham)')

# Plot for spam messages
sns.histplot(data[data['Label'] == 1]['num_sentences'], ax=axes[1])
axes[1].set_xlabel('Number of Sentences')
axes[1].set_ylabel('Frequency')
axes[1].set_title('Histogram of Number of Sentences (Spam)')

plt.tight_layout()
plt.show()

"""Correlation"""

data

df=data.drop("Text",axis=1)

df.corr()

#visualize correlation
sns.heatmap(df.corr(),annot=True)

"""Text Pre-processing"""

data.head()

from nltk.corpus import stopwords
stopwords.words('english')

!pip install nltk # Make sure NLTK is installed.
import nltk

nltk.download('stopwords') # Download the stopwords resource.

from nltk.corpus import stopwords
stopwords.words('english')

import string
string.punctuation

from nltk.stem.porter import PorterStemmer
ps = PorterStemmer()

def transform_text(text):
    text = text.lower()
    text = nltk.word_tokenize(text)

    y=[]
    for i in text:
        if i.isalnum() and i not in stopwords.words('english') and i not in string.punctuation:
            y.append(i)
    text.clear()
    text = []
    for i in y:
        text.append(ps.stem(i))
    return text

data['transformed_text'] = data['Text'].apply(transform_text)

data.head()

"""Word Cloud"""

pip install wordcloud

from wordcloud import WordCloud
wc = WordCloud(width=500, height=500, min_font_size=10, background_color='white')

data['transformed_text'] = data['transformed_text'].astype(str)
spam_wc = wc.generate(data[data['Label'] == 1]['transformed_text'].str.cat(sep=" "))

plt.imshow(spam_wc)

ham_wc = wc.generate(data[data['Label'] == 0]['transformed_text'].str.cat(sep=" "))

plt.imshow(ham_wc)

"""Lets find top 25 words from both ham and spam text"""

spam_corpus = []
for msg in data[data['Label']==1]['transformed_text'].tolist():
    for word in msg.split():
        spam_corpus.append(word)

ham_corpus = []
for msg in data[data['Label']==0]['transformed_text'].tolist():
    for word in msg.split():
        ham_corpus.append(word)

from collections import Counter

spam_corpus = data[data['Label'] == 1]['transformed_text'].str.cat(sep=" ").split()
most_common_words = pd.DataFrame(Counter(spam_corpus).most_common(30), columns=['word', 'count'])
sns.barplot(x='word', y='count', data=most_common_words)
plt.xticks(rotation="vertical")
plt.show()

ham_corpus = data[data['Label'] == 0]['transformed_text'].str.cat(sep=" ").split()
most_common_words_ham = pd.DataFrame(Counter(ham_corpus).most_common(30), columns=['word', 'count'])
sns.barplot(x='word', y='count', data=most_common_words_ham)
plt.xticks(rotation="vertical")
plt.show()

"""Model Building"""

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
cv = CountVectorizer()
tfidf = TfidfVectorizer()

X = tfidf.fit_transform(data['transformed_text']).toarray()

X.shape

y = data['Label'].values

y.shape

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.metrics import precision_score

X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.2, random_state=2)

from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import BaggingClassifier
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.ensemble import GradientBoostingClassifier

svc = SVC(kernel='sigmoid', gamma=1.0)
knc = KNeighborsClassifier()
mnb = MultinomialNB()
dtc = DecisionTreeClassifier(max_depth=5)
lrc = LogisticRegression(solver='liblinear', penalty='l1')
rfc = RandomForestClassifier(n_estimators=50, random_state=2)
abc = AdaBoostClassifier(n_estimators=50, random_state=2)
bc = BaggingClassifier(n_estimators=50, random_state=2)
etc = ExtraTreesClassifier(n_estimators=50, random_state=2)
gbdt = GradientBoostingClassifier(n_estimators=50,random_state=2)

clfs = {
    'SVC' : svc,
    'KN' : knc,
    'NB': mnb,
    'DT': dtc,
    'LR': lrc,
    'RF': rfc,
    'AdaBoost': abc,
    'BgC': bc,
    'ETC': etc,
    'GBDT':gbdt,
}

def train_classifier(clf,X_train,y_train,X_test,y_test):
    clf.fit(X_train,y_train)
    y_pred = clf.predict(X_test)
    accuracy = accuracy_score(y_test,y_pred)
    precision = precision_score(y_test,y_pred)

    return accuracy,precision

accuracy_scores=[]
precision_scores=[]

for name,clf in clfs.items():

    current_accuracy,current_precision = train_classifier(clf, X_train,y_train,X_test,y_test)

    print("For ",name)
    print("Accuracy - ",current_accuracy)
    print("Precision - ",current_precision)

    accuracy_scores.append(current_accuracy)
    precision_scores.append(current_precision)

performance_df = pd.DataFrame({'Algorithm': clfs.keys(), 'Accuracy': accuracy_scores, 'Precision': precision_scores})
performance_df

performance_df = performance_df.sort_values('Precision', ascending=False)
performance_df

